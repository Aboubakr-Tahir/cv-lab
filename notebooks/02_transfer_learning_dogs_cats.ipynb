{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to upload your kaggle.json file\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Prompt to upload the file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Set up the Kaggle directory and permissions\n",
        "if 'kaggle.json' in uploaded:\n",
        "    !mkdir -p ~/.kaggle\n",
        "    !mv kaggle.json ~/.kaggle/\n",
        "    !chmod 600 ~/.kaggle/kaggle.json\n",
        "    print(\"Kaggle API token configured successfully!\")\n",
        "else:\n",
        "    print(\"kaggle.json not found. Please upload the file.\")"
      ],
      "metadata": {
        "id": "jozd6SumwSNF"
      },
      "id": "jozd6SumwSNF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub datasets -q"
      ],
      "metadata": {
        "id": "WClaGUpP0aGZ"
      },
      "id": "WClaGUpP0aGZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "59Xhhvp-sDN-"
      },
      "id": "59Xhhvp-sDN-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "gbIzdC1aspwP"
      },
      "id": "gbIzdC1aspwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Downloading dataset from Kaggle...\")\n",
        "# This downloads the files and returns the root path where they are stored\n",
        "dataset_path = kagglehub.dataset_download(\"shaunthesheep/microsoft-catsvsdogs-dataset\")\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")\n",
        "\n",
        "# 3. Construct the full path to the image folder\n",
        "# The images are inside the 'PetImages' subdirectory\n",
        "image_folder_path = os.path.join(dataset_path, \"PetImages\")\n",
        "\n",
        "# 4. Load the dataset using the \"imagefolder\" builder from Hugging Face\n",
        "# This tells the library to treat the directory as an image dataset\n",
        "print(\"Loading images into Hugging Face Dataset...\")\n",
        "hf_dataset = load_dataset(\"imagefolder\", data_dir=image_folder_path)\n",
        "\n",
        "print(\"\\n✅ Successfully loaded as a Hugging Face Dataset:\")\n",
        "print(hf_dataset)"
      ],
      "metadata": {
        "id": "ppEHuaWZ0dWz"
      },
      "id": "ppEHuaWZ0dWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "original_data_dir = '/kaggle/input/microsoft-catsvsdogs-dataset/PetImages'\n",
        "copied_data_dir = '/kaggle/working/PetImages_copy'\n",
        "\n",
        "# Remove the copied directory if it already exists to start fresh\n",
        "if os.path.exists(copied_data_dir):\n",
        "    shutil.rmtree(copied_data_dir)\n",
        "\n",
        "print(\"Copying dataset to a writable directory...\")\n",
        "shutil.copytree(original_data_dir, copied_data_dir)\n",
        "print(\"Copying complete.\")"
      ],
      "metadata": {
        "id": "4uN4KnK3EnuJ"
      },
      "id": "4uN4KnK3EnuJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Define the path to your main PetImages directory\n",
        "data_dir = '/kaggle/working/PetImages_copy'\n",
        "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
        "\n",
        "# Get the class names (Cat, Dog)\n",
        "class_names = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "# Loop through each class directory\n",
        "for class_name in class_names:\n",
        "    class_dir = os.path.join(data_dir, class_name)\n",
        "    if not os.path.isdir(class_dir):\n",
        "        continue\n",
        "\n",
        "    # Loop through each image in the class directory\n",
        "    for fname in os.listdir(class_dir):\n",
        "        # Check for valid image extensions\n",
        "        if fname.lower().endswith(tuple(image_extensions)):\n",
        "            image_path = os.path.join(class_dir, fname)\n",
        "            try:\n",
        "                # Try to open the image\n",
        "                with Image.open(image_path) as img:\n",
        "                    img.verify() # Verify the image integrity\n",
        "            except (IOError, SyntaxError, Image.UnidentifiedImageError) as e:\n",
        "                print(f'Corrupted image found and deleted: {image_path}')\n",
        "                os.remove(image_path) # Delete the corrupted file\n",
        "\n",
        "print(\"Dataset cleaning complete.\")"
      ],
      "metadata": {
        "id": "_hjOeteoEGVp"
      },
      "id": "_hjOeteoEGVp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ],
      "metadata": {
        "id": "JUXR8MD_1e2D"
      },
      "id": "JUXR8MD_1e2D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "base_data_dir = '/kaggle/working/PetImages_copy'\n",
        "train_dataset_full = datasets.ImageFolder(base_data_dir, transform=data_transforms['train'])\n",
        "val_dataset_full   = datasets.ImageFolder(base_data_dir, transform=data_transforms['val'])\n",
        "\n",
        "train_size = int(0.8 * len(train_dataset_full))\n",
        "val_size   = len(train_dataset_full) - train_size\n",
        "\n",
        "\n",
        "indices = torch.randperm(len(train_dataset_full))\n",
        "train_idx = indices[:train_size]\n",
        "val_idx   = indices[train_size:]\n",
        "train_dataset = torch.utils.data.Subset(train_dataset_full, train_idx)\n",
        "val_dataset   = torch.utils.data.Subset(val_dataset_full, val_idx)\n",
        "\n",
        "# --- 5. Apply the VAL Transforms to the Validation Subset ---\n",
        "# Since random_split creates a Subset object, we can override its transform.\n",
        "# This ensures validation data is not augmented with RandomResizedCrop/Flip.\n",
        "val_dataset.dataset.transform = data_transforms['val']\n",
        "\n",
        "\n",
        "# --- 6. Create the Dataloaders ---\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4),\n",
        "    'val': DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4) # typically no shuffle for val\n",
        "}\n",
        "\n",
        "# --- 7. Update Dataset Sizes and Class Names ---\n",
        "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
        "class_names = train_dataset_full.classes\n",
        "\n",
        "print(\"Class Names:\", class_names)\n",
        "print(\"Training Size:\", dataset_sizes['train'])\n",
        "print(\"Validation Size:\", dataset_sizes['val'])\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "x8cJric23g2Y"
      },
      "id": "x8cJric23g2Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def show_batch(dataloader, class_names, n_images=16, title=\"Sample Training Images\"):\n",
        "    \"\"\"Display a batch of images with their labels in a clean grid.\"\"\"\n",
        "    # Get one batch\n",
        "    inputs, classes = next(iter(dataloader))\n",
        "\n",
        "    # Make sure we don’t go over the number of images in the batch\n",
        "    n_images = min(n_images, inputs.size(0))\n",
        "\n",
        "    # Create a figure\n",
        "    cols = 4\n",
        "    rows = n_images // cols + int(n_images % cols != 0)\n",
        "    plt.figure(figsize=(cols * 3, rows * 3))\n",
        "    plt.suptitle(title, fontsize=16)\n",
        "\n",
        "    # Unnormalize function\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    for i in range(n_images):\n",
        "        ax = plt.subplot(rows, cols, i + 1)\n",
        "        img = inputs[i].numpy().transpose((1, 2, 0))\n",
        "        img = std * img + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(class_names[classes[i]], fontsize=10)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "show_batch(dataloaders['train'], class_names, n_images=16, title=\"Training Batch (with Augmentations)\")\n"
      ],
      "metadata": {
        "id": "JF2O87K-Kyue"
      },
      "id": "JF2O87K-Kyue",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb # Make sure wandb is imported\n",
        "import time\n",
        "from tempfile import TemporaryDirectory\n",
        "import torch\n",
        "\n",
        "import copy # Make sure to import the copy library\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Keep the best model weights in memory instead of a file\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    wandb.watch(model, log=\"all\", log_freq=10)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "            if phase == 'train':\n",
        "                wandb.log({\"train_acc\": epoch_acc, \"train_loss\": epoch_loss, \"epoch\": epoch})\n",
        "            else:\n",
        "                wandb.log({\"val_acc\": epoch_acc, \"val_loss\": epoch_loss, \"epoch\": epoch})\n",
        "\n",
        "            # If this is the best model so far, save its weights in our variable\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "    print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "    # Load the best model weights from our variable\n",
        "    model.load_state_dict(best_model_wts)\n",
        "\n",
        "    wandb.finish()\n",
        "    return model"
      ],
      "metadata": {
        "id": "MPLGyFiALyMm"
      },
      "id": "MPLGyFiALyMm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_model(model, dataloader, class_names, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plt.suptitle(\"Model Predictions\", fontsize=16)\n",
        "\n",
        "    # Unnormalize function\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "    with torch.no_grad(): # Turn off gradients for prediction\n",
        "        for i, (inputs, labels) in enumerate(dataloader):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "\n",
        "                # Format the title to show prediction and true label\n",
        "                ax.set_title(f'Predicted: {class_names[preds[j]]} | True: {class_names[labels[j]]}')\n",
        "\n",
        "                # Display the image\n",
        "                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
        "                img = std * img + mean\n",
        "                img = np.clip(img, 0, 1)\n",
        "                plt.imshow(img)\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training) # Set model back to original mode\n",
        "                    plt.tight_layout()\n",
        "                    plt.show()\n",
        "                    return\n",
        "    model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "9GhneWFF02H_"
      },
      "id": "9GhneWFF02H_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft = model_ft.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "wandb.init(\n",
        "    project=\"cats-vs-dogs-resnet\",\n",
        "    config={\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"architecture\": \"ResNet18\",\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"momentum\": 0.9,\n",
        "        \"lr_scheduler\": \"StepLR\",\n",
        "        \"scheduler_step_size\": 7,\n",
        "        \"scheduler_gamma\": 0.1,\n",
        "        \"loss_function\": \"CrossEntropyLoss\",\n",
        "        \"epochs\": 25,\n",
        "    }\n",
        ")\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=25)"
      ],
      "metadata": {
        "id": "Ro9eny7L32uw"
      },
      "id": "Ro9eny7L32uw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model_ft, dataloaders['val'], class_names)"
      ],
      "metadata": {
        "id": "mecoTev_YlVJ"
      },
      "id": "mecoTev_YlVJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
        "\n",
        "wandb.init(\n",
        "    project=\"cats-vs-dogs-resnet\",\n",
        "\n",
        "    # Give the run a specific name so you can easily identify it\n",
        "    name=\"feature_extraction_run\",\n",
        "\n",
        "    config={\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"architecture\": \"ResNet18\",\n",
        "\n",
        "        # --- NEW: Add the training strategy ---\n",
        "        \"training_strategy\": \"Feature Extraction (Frozen Layers)\",\n",
        "\n",
        "        \"optimizer\": \"SGD\",\n",
        "        \"momentum\": 0.9,\n",
        "        \"lr_scheduler\": \"StepLR\",\n",
        "        \"scheduler_step_size\": 7,\n",
        "        \"scheduler_gamma\": 0.1,\n",
        "        \"loss_function\": \"CrossEntropyLoss\",\n",
        "        \"epochs\": 25,\n",
        "    }\n",
        ")\n",
        "\n",
        "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
        "                         exp_lr_scheduler, num_epochs=25)"
      ],
      "metadata": {
        "id": "PZ7UmgT1acO0"
      },
      "id": "PZ7UmgT1acO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model_conv, dataloaders['val'], class_names)"
      ],
      "metadata": {
        "id": "Kpb5gdHsTPg7"
      },
      "id": "Kpb5gdHsTPg7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}